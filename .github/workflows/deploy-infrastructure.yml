name: ğŸš€ Deploy Infrastructure

on:
  push:
    branches: [main, production]
    paths:
      - '**.tf'
      - '**.tfvars'
      - 'website/**'
      - '.github/workflows/deploy-infrastructure.yml'
  pull_request:
    branches: [main, production]
    paths:
      - '**.tf'
      - '**.tfvars'
  workflow_dispatch:
    inputs:
      action:
        description: 'Action to perform'
        required: true
        default: 'apply'
        type: choice
        options:
          - plan
          - apply

env:
  TF_VERSION: '1.5.0'
  AWS_REGION: 'us-east-1'
  TF_IN_AUTOMATION: true
  TF_INPUT: false
  TF_WORKING_DIR: '.'

permissions:
  contents: read
  pull-requests: write
  security-events: write

jobs:
  setup:
    name: ğŸ” Detect Project Structure
    runs-on: ubuntu-latest
    outputs:
      tf_dir: ${{ steps.detect.outputs.tf_dir }}
    
    steps:
      - name: ğŸ“¥ Checkout Code
        uses: actions/checkout@v4

      - name: ğŸ” Detect Terraform Directory
        id: detect
        run: |
          echo "ğŸ” Detecting Terraform files location..."
          
          if [ -f "main.tf" ]; then
            echo "âœ… Found Terraform files in root directory"
            echo "tf_dir=." >> $GITHUB_OUTPUT
          elif [ -f "terraform/main.tf" ]; then
            echo "âœ… Found Terraform files in terraform/ subdirectory"
            echo "tf_dir=./terraform" >> $GITHUB_OUTPUT
          else
            echo "âŒ ERROR: Cannot find main.tf in root or terraform/ directory"
            echo "ğŸ“‚ Current directory structure:"
            ls -la
            echo ""
            echo "ğŸ“‚ Looking for .tf files:"
            find . -name "*.tf" -type f
            exit 1
          fi

  validate:
    name: ğŸ” Validate Terraform
    runs-on: ubuntu-latest
    needs: setup
    outputs:
      tf-fmt: ${{ steps.fmt.outcome }}
      tf-validate: ${{ steps.validate.outcome }}
    
    steps:
      - name: ğŸ“¥ Checkout Code
        uses: actions/checkout@v4

      - name: ğŸ”§ Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}

      - name: ğŸ“‚ Show Structure
        run: |
          echo "Working directory: ${{ needs.setup.outputs.tf_dir }}"
          ls -la ${{ needs.setup.outputs.tf_dir }}

      - name: ğŸ¨ Terraform Format Check
        id: fmt
        working-directory: ${{ needs.setup.outputs.tf_dir }}
        run: terraform fmt -check -recursive
        continue-on-error: true

      - name: ğŸ”§ Terraform Init
        working-directory: ${{ needs.setup.outputs.tf_dir }}
        run: terraform init -backend=false

      - name: âœ… Terraform Validate
        id: validate
        working-directory: ${{ needs.setup.outputs.tf_dir }}
        run: terraform validate

      - name: ğŸ“Š Comment PR - Validation
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        continue-on-error: true
        with:
          script: |
            const output = `
            ## ğŸ” Terraform Validation Results
            
            **Working Directory:** \`${{ needs.setup.outputs.tf_dir }}\`
            
            | Check | Result |
            |-------|--------|
            | Format | ${{ steps.fmt.outcome == 'success' && 'âœ… Passed' || 'âš ï¸ Failed' }} |
            | Validate | ${{ steps.validate.outcome == 'success' && 'âœ… Passed' || 'âŒ Failed' }} |
            `;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: output
            });

  security-scan:
    name: ğŸ”’ Security Scan
    runs-on: ubuntu-latest
    needs: [setup, validate]
    
    steps:
      - name: ğŸ“¥ Checkout Code
        uses: actions/checkout@v4

      - name: ğŸ” Run Checkov
        uses: bridgecrewio/checkov-action@master
        with:
          directory: ${{ needs.setup.outputs.tf_dir }}
          framework: terraform
          soft_fail: true
        continue-on-error: true

  plan:
    name: ğŸ“‹ Terraform Plan
    runs-on: ubuntu-latest
    needs: [setup, validate, security-scan]
    if: github.event_name == 'pull_request' || (github.event_name == 'workflow_dispatch' && github.event.inputs.action == 'plan')
    environment: 
      name: production
    
    steps:
      - name: ğŸ“¥ Checkout Code
        uses: actions/checkout@v4

      - name: ğŸ”§ Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}
          terraform_wrapper: false

      - name: ğŸ” Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: ğŸ“‚ Create terraform.tfvars
        working-directory: ${{ needs.setup.outputs.tf_dir }}
        run: |
          if [ ! -f terraform.tfvars ]; then
            if [ -f terraform.tfvars.example ]; then
              cp terraform.tfvars.example terraform.tfvars
            else
              cat > terraform.tfvars << 'EOF'
          use_custom_domain = false
          domain_name       = ""
          project_name      = "secure-website"
          environment       = "production"
          allowed_countries = ["US", "CA", "GB"]
          EOF
            fi
          fi

      - name: ğŸ”§ Terraform Init
        working-directory: ${{ needs.setup.outputs.tf_dir }}
        run: terraform init

      - name: ğŸ“‹ Terraform Plan
        working-directory: ${{ needs.setup.outputs.tf_dir }}
        run: terraform plan -out=tfplan

      - name: ğŸ’¾ Upload Plan
        uses: actions/upload-artifact@v4
        with:
          name: tfplan
          path: ${{ needs.setup.outputs.tf_dir }}/tfplan
          retention-days: 5

      - name: ğŸ“Š Comment PR - Plan
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        continue-on-error: true
        with:
          script: |
            const output = `
            ## ğŸ“‹ Terraform Plan Results
            
            âœ… Plan generated successfully
            
            **Next Steps:**
            - Review the plan carefully
            - Merge to \`main\` to apply changes automatically
            - Or use \`workflow_dispatch\` with \`apply\` action
            `;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: output
            });

  apply:
    name: ğŸš€ Terraform Apply
    runs-on: ubuntu-latest
    needs: [setup, validate, security-scan]
    if: |
      (github.ref == 'refs/heads/main' && github.event_name == 'push') ||
      (github.event_name == 'workflow_dispatch' && github.event.inputs.action == 'apply')
    environment: 
      name: production
    
    steps:
      - name: ğŸ“¥ Checkout Code
        uses: actions/checkout@v4

      - name: ğŸ”§ Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}
          terraform_wrapper: false

      - name: ğŸ” Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: ğŸ“‚ Create terraform.tfvars
        working-directory: ${{ needs.setup.outputs.tf_dir }}
        run: |
          if [ ! -f terraform.tfvars ]; then
            if [ -f terraform.tfvars.example ]; then
              cp terraform.tfvars.example terraform.tfvars
            else
              cat > terraform.tfvars << 'EOF'
          use_custom_domain = false
          domain_name       = ""
          project_name      = "secure-website"
          environment       = "production"
          allowed_countries = ["US", "CA", "GB"]
          EOF
            fi
          fi

      - name: ğŸ”§ Terraform Init
        working-directory: ${{ needs.setup.outputs.tf_dir }}
        run: terraform init

      - name: ğŸ§¹ Smart Cleanup - Remove Orphaned Resources
        working-directory: ${{ needs.setup.outputs.tf_dir }}
        continue-on-error: true
        run: |
          set +e  # Don't exit on errors during cleanup
          
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "ğŸ§¹ SMART CLEANUP - Removing Orphaned Resources"
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo ""
          echo "This prevents duplicate resources from accumulating"
          echo ""
          
          # Get target resource names
          PROJECT=$(terraform console <<< "var.project_name" 2>/dev/null | tr -d '"' || echo "secure-website")
          ENV=$(terraform console <<< "var.environment" 2>/dev/null | tr -d '"' || echo "production")
          TARGET_PREFIX="${PROJECT}-${ENV}"
          
          echo "ğŸ¯ Target resource prefix: $TARGET_PREFIX"
          echo ""
          
          # Check if resource is managed by Terraform
          is_managed() {
            terraform state list 2>/dev/null | grep -q "$1"
          }
          
          # ================================================================
          # 1. CLEANUP ORPHANED CLOUDFRONT DISTRIBUTIONS
          # ================================================================
          echo "â˜ï¸  Scanning CloudFront Distributions..."
          
          DIST_COUNT=0
          aws cloudfront list-distributions --output json 2>/dev/null | \
          jq -r '.DistributionList.Items[]? | select(.Comment | contains("secure-website")) | "\(.Id)|\(.Comment)|\(.Status)|\(.Enabled)"' | \
          while IFS='|' read -r dist_id comment status enabled; do
            DIST_COUNT=$((DIST_COUNT + 1))
            
            # Skip if this is our current managed distribution
            if [ "$comment" = "${TARGET_PREFIX} distribution" ] && is_managed "aws_cloudfront_distribution.main"; then
              echo "  âœ… $dist_id - Current distribution (managed)"
              continue
            fi
            
            # This is an orphaned distribution
            echo "  ğŸ—‘ï¸  Found orphaned: $dist_id ($comment)"
            
            if [ "$enabled" = "true" ]; then
              echo "     Disabling distribution..."
              
              aws cloudfront get-distribution-config --id "$dist_id" > /tmp/cf_$dist_id.json 2>/dev/null
              ETAG=$(cat /tmp/cf_$dist_id.json | jq -r '.ETag')
              
              cat /tmp/cf_$dist_id.json | jq '.DistributionConfig.Enabled = false' > /tmp/cf_disabled_$dist_id.json
              
              aws cloudfront update-distribution \
                --id "$dist_id" \
                --if-match "$ETAG" \
                --distribution-config "$(cat /tmp/cf_disabled_$dist_id.json | jq -c '.DistributionConfig')" \
                2>/dev/null && echo "     âœ… Disabled (will auto-delete after deployment)" || echo "     âš ï¸  Could not disable"
            fi
          done
          
          # ================================================================
          # 2. CLEANUP ORPHANED WAF WEB ACLs
          # ================================================================
          echo ""
          echo "ğŸ›¡ï¸  Scanning WAF Web ACLs..."
          
          aws wafv2 list-web-acls --scope CLOUDFRONT --region us-east-1 --output json 2>/dev/null | \
          jq -r '.WebACLs[]? | select(.Name | contains("secure-website")) | "\(.Id)|\(.Name)|\(.LockToken)"' | \
          while IFS='|' read -r waf_id waf_name lock_token; do
            # Skip if this is our current managed WAF
            if [ "$waf_name" = "${TARGET_PREFIX}-waf" ] && is_managed "aws_wafv2_web_acl.website"; then
              echo "  âœ… $waf_name - Current WAF (managed)"
              continue
            fi
            
            # This is an orphaned WAF
            echo "  ğŸ—‘ï¸  Found orphaned: $waf_name"
            
            # Check if attached to any distribution
            ATTACHED=$(aws cloudfront list-distributions --output json 2>/dev/null | \
                      jq -r ".DistributionList.Items[]? | select(.WebACLId | contains(\"$waf_id\")) | .Id" | head -1)
            
            if [ -z "$ATTACHED" ]; then
              echo "     Deleting unattached WAF..."
              aws wafv2 delete-web-acl \
                --scope CLOUDFRONT \
                --id "$waf_id" \
                --lock-token "$lock_token" \
                --region us-east-1 2>&1 && \
              echo "     âœ… Deleted" || \
              echo "     âš ï¸  Could not delete (may be in use)"
            else
              echo "     âš ï¸  Still attached to CloudFront - will clean after distribution removal"
            fi
          done
          
          # ================================================================
          # 3. CLEANUP ORPHANED S3 BUCKETS
          # ================================================================
          echo ""
          echo "ğŸ“¦ Scanning S3 Buckets..."

          aws s3api list-buckets --query 'Buckets[].Name' --output text 2>/dev/null | \
          tr '\t' '\n' | grep "secure-website" | \
          while read -r bucket; do
            # Skip if this is our current managed bucket
            if [ "$bucket" = "${TARGET_PREFIX}-website" ] && is_managed "aws_s3_bucket.website"; then
              echo "  âœ… $bucket - Current website bucket (managed)"
              continue
            fi
            
            if [ "$bucket" = "${TARGET_PREFIX}-logs" ] && is_managed "aws_s3_bucket.logs"; then
              echo "  âœ… $bucket - Current logs bucket (managed)"
              continue
            fi
            
            # This is an orphaned bucket
            echo "  ğŸ—‘ï¸  Found orphaned: $bucket"
            echo "     Emptying completely..."
            
            # Method 1: Delete all objects and versions
            aws s3api delete-objects \
              --bucket $bucket \
              --delete "$(aws s3api list-object-versions \
                --bucket $bucket \
                --output json \
                --query='{Objects: Versions[].{Key:Key,VersionId:VersionId}}')" 2>/dev/null || true
            
            # Method 2: Delete all delete markers
            aws s3api delete-objects \
              --bucket $bucket \
              --delete "$(aws s3api list-object-versions \
                --bucket $bucket \
                --output json \
                --query='{Objects: DeleteMarkers[].{Key:Key,VersionId:VersionId}}')" 2>/dev/null || true
            
            # Method 3: Regular delete as fallback
            aws s3 rm s3://$bucket --recursive --quiet 2>/dev/null || true
            
            echo "     Deleting bucket..."
            aws s3 rb s3://$bucket --force 2>&1 && \
            echo "     âœ… Deleted" || \
            echo "     âš ï¸  Could not delete (may have pending operations)"
          done
          
          # ================================================================
          # 4. CLEANUP ORPHANED CLOUDWATCH DASHBOARDS
          # ================================================================
          echo ""
          echo "ğŸ“Š Scanning CloudWatch Dashboards..."
          
          aws cloudwatch list-dashboards --region us-east-1 --output json 2>/dev/null | \
          jq -r '.DashboardEntries[]? | select(.DashboardName | contains("secure-website")) | .DashboardName' | \
          while read -r dashboard; do
            # Skip if this is our current managed dashboard
            if [ "$dashboard" = "${TARGET_PREFIX}-dashboard" ] && is_managed "aws_cloudwatch_dashboard.main"; then
              echo "  âœ… $dashboard - Current dashboard (managed)"
              continue
            fi
            
            # This is an orphaned dashboard
            echo "  ğŸ—‘ï¸  Found orphaned: $dashboard"
            aws cloudwatch delete-dashboards --dashboard-names "$dashboard" --region us-east-1 2>&1 && \
            echo "     âœ… Deleted" || \
            echo "     âš ï¸  Could not delete"
          done

          # ================================================================
          # 5. CLEANUP ORPHANED CLOUDWATCH LOG GROUPS
          # ================================================================
          echo ""
          echo "ğŸ“ Scanning CloudWatch Log Groups..."

          aws logs describe-log-groups --log-group-name-prefix "aws-waf-logs-${TARGET_PREFIX}" --query 'logGroups[].logGroupName' --output text 2>/dev/null | \
          tr '\t' '\n' | \
          while read -r log_group; do
            # Skip if this is our current managed log group
            if [ "$log_group" = "aws-waf-logs-${TARGET_PREFIX}" ] && is_managed "aws_cloudwatch_log_group.waf_logs"; then
              echo "  âœ… $log_group - Current log group (managed)"
              continue
            fi
            
            # This is an orphaned log group
            echo "  ğŸ—‘ï¸  Found orphaned: $log_group"
            aws logs delete-log-group --log-group-name "$log_group" 2>&1 && \
            echo "     âœ… Deleted" || \
            echo "     âš ï¸  Could not delete"
          done

          # ================================================================
          # 6. CLEANUP ORPHANED IAM ROLES
          # ================================================================
          echo ""
          echo "ğŸ‘¤ Scanning IAM Roles..."

          aws iam list-roles --query 'Roles[].RoleName' --output text 2>/dev/null | \
          tr '\t' '\n' | grep "${TARGET_PREFIX}" | \
          while read -r role; do
            # Skip if this is our current managed role
            if [ "$role" = "${TARGET_PREFIX}-firehose-role" ] && is_managed "aws_iam_role.firehose_delivery_role"; then
              echo "  âœ… $role - Current IAM role (managed)"
              continue
            fi
            
            # This is an orphaned role
            echo "  ğŸ—‘ï¸  Found orphaned: $role"
            
            # Delete inline policies first
            echo "     Removing inline policies..."
            aws iam list-role-policies --role-name "$role" --query 'PolicyNames' --output text 2>/dev/null | \
            tr '\t' '\n' | while read -r policy; do
              aws iam delete-role-policy --role-name "$role" --policy-name "$policy" 2>/dev/null && \
              echo "       âœ… Deleted policy: $policy" || \
              echo "       âš ï¸  Could not delete policy: $policy"
            done
            
            # Delete attached policies
            echo "     Detaching managed policies..."
            aws iam list-attached-role-policies --role-name "$role" --query 'AttachedPolicies[].PolicyArn' --output text 2>/dev/null | \
            tr '\t' '\n' | while read -r policy_arn; do
              aws iam detach-role-policy --role-name "$role" --policy-arn "$policy_arn" 2>/dev/null && \
              echo "       âœ… Detached policy: $(basename $policy_arn)" || \
              echo "       âš ï¸  Could not detach policy: $(basename $policy_arn)"
            done
            
            # Delete the role
            echo "     Deleting role..."
            aws iam delete-role --role-name "$role" 2>&1 && \
            echo "     âœ… Deleted" || \
            echo "     âš ï¸  Could not delete"
          done

          # ================================================================
          # 7. CLEANUP ORPHANED CLOUDFRONT OAC
          # ================================================================
          echo ""
          echo "ğŸ”’ Scanning CloudFront Origin Access Controls..."

          aws cloudfront list-origin-access-controls --query 'OriginAccessControlList.Items[].Name' --output text 2>/dev/null | \
          tr '\t' '\n' | grep "${TARGET_PREFIX}" | \
          while read -r oac_name; do
            # Skip if this is our current managed OAC
            if [ "$oac_name" = "${TARGET_PREFIX}-oac" ] && is_managed "aws_cloudfront_origin_access_control.website"; then
              echo "  âœ… $oac_name - Current OAC (managed)"
              continue
            fi
            
            # This is an orphaned OAC
            echo "  ğŸ—‘ï¸  Found orphaned: $oac_name"
            
            # Get OAC ID and delete
            OAC_ID=$(aws cloudfront list-origin-access-controls --query "OriginAccessControlList.Items[?Name=='$oac_name'].Id" --output text 2>/dev/null)
            if [ -n "$OAC_ID" ]; then
              aws cloudfront delete-origin-access-control --id "$OAC_ID" 2>&1 && \
              echo "     âœ… Deleted" || \
              echo "     âš ï¸  Could not delete"
            fi
          done

          # ================================================================
          # 8. CLEANUP ORPHANED CLOUDFRONT RESPONSE HEADERS POLICIES
          # ================================================================
          echo ""
          echo "ğŸ“‹ Scanning CloudFront Response Headers Policies..."

          aws cloudfront list-response-headers-policies --query 'ResponseHeadersPolicyList.Items[].Name' --output text 2>/dev/null | \
          tr '\t' '\n' | grep "${TARGET_PREFIX}" | \
          while read -r policy_name; do
            # Skip if this is our current managed policy
            if [ "$policy_name" = "${TARGET_PREFIX}-security-headers" ] && is_managed "aws_cloudfront_response_headers_policy.security_headers"; then
              echo "  âœ… $policy_name - Current headers policy (managed)"
              continue
            fi
            
            # This is an orphaned policy
            echo "  ğŸ—‘ï¸  Found orphaned: $policy_name"
            
            # Get policy ID and delete
            POLICY_ID=$(aws cloudfront list-response-headers-policies --query "ResponseHeadersPolicyList.Items[?Name=='$policy_name'].Id" --output text 2>/dev/null)
            if [ -n "$POLICY_ID" ]; then
              aws cloudfront delete-response-headers-policy --id "$POLICY_ID" 2>&1 && \
              echo "     âœ… Deleted" || \
              echo "     âš ï¸  Could not delete"
            fi
          done

          # ================================================================
          # 9. CLEANUP ORPHANED KINESIS FIREHOSE DELIVERY STREAMS
          # ================================================================
          echo ""
          echo "ğŸ”¥ Scanning Kinesis Firehose Delivery Streams..."

          aws firehose list-delivery-streams --query 'DeliveryStreamNames' --output text 2>/dev/null | \
          tr '\t' '\n' | grep "${TARGET_PREFIX}" | \
          while read -r stream; do
            # Skip if this is our current managed firehose
            if [ "$stream" = "aws-waf-logs-${TARGET_PREFIX}" ] && is_managed "aws_kinesis_firehose_delivery_stream.waf_logs"; then
              echo "  âœ… $stream - Current firehose (managed)"
              continue
            fi
            
            # This is an orphaned firehose
            echo "  ğŸ—‘ï¸  Found orphaned: $stream"
            aws firehose delete-delivery-stream --delivery-stream-name "$stream" 2>&1 && \
            echo "     âœ… Deleted" || \
            echo "     âš ï¸  Could not delete"
          done
          
          echo ""
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "âœ… Cleanup scan complete!"
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo ""
          
          set -e  # Re-enable exit on error

      - name: ğŸ” Detect WAF Resource Name
        id: detect_waf
        working-directory: ${{ needs.setup.outputs.tf_dir }}
        run: |
          if grep -r "resource \"aws_wafv2_web_acl\" \"website\"" . >/dev/null 2>&1; then
            echo "waf_resource=aws_wafv2_web_acl.website" >> $GITHUB_OUTPUT
            echo "âœ… Detected: aws_wafv2_web_acl.website"
          elif grep -r "resource \"aws_wafv2_web_acl\" \"main\"" . >/dev/null 2>&1; then
            echo "waf_resource=aws_wafv2_web_acl.main" >> $GITHUB_OUTPUT
            echo "âœ… Detected: aws_wafv2_web_acl.main"
          else
            echo "waf_resource=aws_wafv2_web_acl.website" >> $GITHUB_OUTPUT
            echo "âš ï¸  Using default: aws_wafv2_web_acl.website"
          fi

      - name: ğŸ§  Smart Resource Import
        working-directory: ${{ needs.setup.outputs.tf_dir }}
        env:
          WAF_RESOURCE: ${{ steps.detect_waf.outputs.waf_resource }}
        run: |
          set +e  # Don't exit on import errors
          
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "ğŸ§  SMART RESOURCE IMPORT - Reuse Existing Resources"
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo ""
          
          PROJECT=$(terraform console <<< "var.project_name" 2>/dev/null | tr -d '"' || echo "secure-website")
          ENV=$(terraform console <<< "var.environment" 2>/dev/null | tr -d '"' || echo "production")
          PREFIX="${PROJECT}-${ENV}"
          
          echo "ğŸ“‹ Configuration:"
          echo "   Project: $PROJECT"
          echo "   Environment: $ENV"
          echo "   Prefix: $PREFIX"
          echo "   WAF Resource: $WAF_RESOURCE"
          echo ""
          
          # Smart import function
          smart_import() {
            local tf_resource=$1
            local aws_id=$2
            local name=$3
            
            if [ -z "$aws_id" ] || [ "$aws_id" = "null" ]; then
              echo "  â„¹ï¸  $name - Not found in AWS (will be created)"
              return 1
            fi
            
            if terraform state show "$tf_resource" &>/dev/null; then
              echo "  âœ… $name - Already managed"
              return 0
            fi
            
            echo "  ğŸ”„ $name - Importing..."
            if terraform import "$tf_resource" "$aws_id" 2>&1 | tee /tmp/import_$name.log; then
              echo "  âœ… $name - Imported successfully!"
              return 0
            else
              if grep -qi "already exists\|duplicate" /tmp/import_$name.log; then
                echo "  âš ï¸  $name - Exists but cannot import (will handle automatically)"
              else
                echo "  â„¹ï¸  $name - Will be created"
              fi
              return 1
            fi
          }
          
          # Import S3 Buckets
          echo "ğŸ“¦ S3 Buckets:"
          smart_import "aws_s3_bucket.website" "${PREFIX}-website" "Website Bucket"
          smart_import "aws_s3_bucket.logs" "${PREFIX}-logs" "Logs Bucket"
          
          # Import WAF with automatic duplicate handling
          echo ""
          echo "ğŸ›¡ï¸  WAF Web ACL:"
          WAF_JSON=$(aws wafv2 list-web-acls --scope CLOUDFRONT --region us-east-1 --output json 2>/dev/null || echo '{"WebACLs":[]}')
          WAF_ID=$(echo "$WAF_JSON" | jq -r ".WebACLs[] | select(.Name==\"${PREFIX}-waf\") | .Id" 2>/dev/null)
          
          if [ -n "$WAF_ID" ] && [ "$WAF_ID" != "null" ]; then
            WAF_IMPORT_ID="${WAF_ID}/${PREFIX}-waf/CLOUDFRONT"
            
            if ! smart_import "$WAF_RESOURCE" "$WAF_IMPORT_ID" "WAF Web ACL"; then
              # Import failed - delete and recreate
              echo "  ğŸ”„ Import failed - removing duplicate WAF..."
              LOCK_TOKEN=$(echo "$WAF_JSON" | jq -r ".WebACLs[] | select(.Name==\"${PREFIX}-waf\") | .LockToken")
              
              if [ -n "$LOCK_TOKEN" ]; then
                # Check if attached to CloudFront
                ATTACHED=$(aws cloudfront list-distributions --output json 2>/dev/null | \
                          jq -r ".DistributionList.Items[]? | select(.WebACLId | contains(\"$WAF_ID\")) | .Id" | head -1)
                
                if [ -n "$ATTACHED" ]; then
                  echo "  ğŸ”„ Detaching from CloudFront $ATTACHED..."
                  aws cloudfront get-distribution-config --id "$ATTACHED" > /tmp/dist_waf.json
                  DIST_ETAG=$(cat /tmp/dist_waf.json | jq -r '.ETag')
                  
                  cat /tmp/dist_waf.json | jq '.DistributionConfig.WebACLId = ""' > /tmp/dist_no_waf.json
                  
                  aws cloudfront update-distribution \
                    --id "$ATTACHED" \
                    --if-match "$DIST_ETAG" \
                    --distribution-config "$(cat /tmp/dist_no_waf.json | jq -c '.DistributionConfig')" 2>/dev/null || true
                  
                  sleep 5
                fi
                
                echo "  ğŸ—‘ï¸  Deleting duplicate WAF..."
                aws wafv2 delete-web-acl \
                  --scope CLOUDFRONT \
                  --id "$WAF_ID" \
                  --lock-token "$LOCK_TOKEN" \
                  --region us-east-1 2>&1 && \
                echo "  âœ… Deleted - Terraform will create new one" || \
                echo "  âš ï¸  Could not delete"
              fi
            fi
          else
            echo "  â„¹ï¸  WAF not found (will be created)"
          fi
          
          # Import CloudFront
          echo ""
          echo "â˜ï¸  CloudFront Distribution:"
          DIST_JSON=$(aws cloudfront list-distributions --output json 2>/dev/null || echo '{"DistributionList":{"Items":[]}}')
          DIST_ID=$(echo "$DIST_JSON" | jq -r ".DistributionList.Items[] | select(.Comment==\"${PREFIX} distribution\") | .Id" 2>/dev/null)
          smart_import "aws_cloudfront_distribution.main" "$DIST_ID" "CloudFront Distribution"
          
          # Import CloudWatch Dashboard
          echo ""
          echo "ğŸ“Š CloudWatch Dashboard:"
          if aws cloudwatch get-dashboard --dashboard-name "${PREFIX}-dashboard" --region us-east-1 &>/dev/null; then
            smart_import "aws_cloudwatch_dashboard.main" "${PREFIX}-dashboard" "Dashboard"
          else
            echo "  â„¹ï¸  Dashboard not found (will be created)"
          fi
          
          echo ""
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "âœ… Import process complete!"
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo ""
          
          set -e

      - name: ğŸ“‹ Terraform Plan
        working-directory: ${{ needs.setup.outputs.tf_dir }}
        run: |
          echo "ğŸ“‹ Creating execution plan..."
          terraform plan -out=tfplan

      - name: ğŸš€ Terraform Apply
        working-directory: ${{ needs.setup.outputs.tf_dir }}
        run: |
          echo "ğŸš€ Applying infrastructure changes..."
          terraform apply -auto-approve tfplan

      - name: ğŸ“¤ Get Outputs
        id: outputs
        working-directory: ${{ needs.setup.outputs.tf_dir }}
        run: |
          echo "website_url=$(terraform output -raw website_url 2>/dev/null || echo 'N/A')" >> $GITHUB_OUTPUT
          echo "cloudfront_domain=$(terraform output -raw cloudfront_domain_name 2>/dev/null || echo 'N/A')" >> $GITHUB_OUTPUT
          echo "s3_bucket=$(terraform output -raw s3_bucket_name 2>/dev/null || echo 'N/A')" >> $GITHUB_OUTPUT
          echo "distribution_id=$(terraform output -raw cloudfront_distribution_id 2>/dev/null || echo 'N/A')" >> $GITHUB_OUTPUT

      - name: ğŸ“ Upload Website Files
        run: |
          BUCKET="${{ steps.outputs.outputs.s3_bucket }}"
          
          if [ "$BUCKET" = "N/A" ]; then
            echo "âš ï¸ Bucket not available"
            exit 0
          fi
          
          # Create default website if needed
          if [ ! -d "website" ]; then
            mkdir -p website
            cat > website/index.html << 'EOF'
          <!DOCTYPE html>
          <html>
          <head>
              <meta charset="UTF-8">
              <meta name="viewport" content="width=device-width, initial-scale=1.0">
              <title>Deployed!</title>
              <style>
                  body {
                      font-family: system-ui, -apple-system, sans-serif;
                      display: flex;
                      align-items: center;
                      justify-content: center;
                      min-height: 100vh;
                      margin: 0;
                      background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                      color: white;
                  }
                  .card {
                      text-align: center;
                      padding: 3rem;
                      background: rgba(255, 255, 255, 0.1);
                      border-radius: 15px;
                      backdrop-filter: blur(10px);
                      box-shadow: 0 8px 32px rgba(0,0,0,0.1);
                  }
                  h1 { margin: 0 0 1rem; font-size: 2.5rem; }
                  p { margin: 0.5rem 0; opacity: 0.9; }
                  .badges { margin: 2rem 0; }
                  .badge {
                      display: inline-block;
                      padding: 0.5rem 1rem;
                      margin: 0.25rem;
                      background: rgba(255, 255, 255, 0.2);
                      border-radius: 20px;
                      font-size: 0.9rem;
                  }
              </style>
          </head>
          <body>
              <div class="card">
                  <h1>ğŸš€ Website Deployed!</h1>
                  <p>Your secure infrastructure is live</p>
                  <div class="badges">
                      <span class="badge">âœ… HTTPS</span>
                      <span class="badge">ğŸ›¡ï¸ WAF Protected</span>
                      <span class="badge">ğŸ“Š Monitored</span>
                      <span class="badge">â˜ï¸ CloudFront</span>
                  </div>
                  <p style="font-size: 0.85rem; margin-top: 2rem;">
                      Build #${{ github.run_number }} â€¢ GitHub Actions
                  </p>
              </div>
          </body>
          </html>
          EOF
          fi
          
          # Sync all files to S3
          echo "ğŸ“ Syncing website files to S3..."
          if [ -d "website" ]; then
            aws s3 sync website/ s3://$BUCKET/ \
              --delete \
              --exclude ".*" \
              --cache-control "public, max-age=3600" \
              --metadata-directive REPLACE
            echo "âœ… Files synced successfully"
          fi

      - name: ğŸ”„ Invalidate CloudFront Cache
        run: |
          DIST_ID="${{ steps.outputs.outputs.distribution_id }}"
          if [ "$DIST_ID" != "N/A" ]; then
            echo "ğŸ”„ Invalidating CloudFront cache..."
            aws cloudfront create-invalidation \
              --distribution-id $DIST_ID \
              --paths "/*"
            echo "âœ… Cache invalidation created"
          fi

      - name: ğŸ“Š Deployment Summary
        run: |
          echo "## ğŸš€ Deployment Complete" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### ğŸŒ Your Website" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**URL:** https://${{ steps.outputs.outputs.cloudfront_domain }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### ğŸ“¦ Resources" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Resource | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|----------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| S3 Bucket | \`${{ steps.outputs.outputs.s3_bucket }}\` |" >> $GITHUB_STEP_SUMMARY
          echo "| Distribution ID | \`${{ steps.outputs.outputs.distribution_id }}\` |" >> $GITHUB_STEP_SUMMARY
          echo "| CloudFront Domain | \`${{ steps.outputs.outputs.cloudfront_domain }}\` |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "ğŸ’¡ **Note:** CloudFront propagation takes 10-15 minutes" >> $GITHUB_STEP_SUMMARY

  monitor:
    name: ğŸ“Š Post-Deployment Monitoring
    runs-on: ubuntu-latest
    needs: [setup, apply]
    if: success()
    
    steps:
      - name: ğŸ“¥ Checkout Code
        uses: actions/checkout@v4

      - name: ğŸ” Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: ğŸ”§ Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}
          terraform_wrapper: false

      - name: ğŸ”§ Terraform Init
        working-directory: ${{ needs.setup.outputs.tf_dir }}
        run: terraform init

      - name: ğŸ“Š Enhanced Deployment Verification
        working-directory: ${{ needs.setup.outputs.tf_dir }}
        run: |
          echo "## ğŸ“Š Deployment Verification" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          DIST=$(terraform output -raw cloudfront_distribution_id 2>/dev/null || echo "N/A")
          DOMAIN=$(terraform output -raw cloudfront_domain_name 2>/dev/null || echo "N/A")
          BUCKET=$(terraform output -raw s3_bucket_name 2>/dev/null || echo "N/A")
          WAF=$(terraform output -raw waf_web_acl_arn 2>/dev/null || echo "N/A")
          DASHBOARD=$(terraform output -raw dashboard_url 2>/dev/null || echo "N/A")
          
          echo "### âœ… Resources Status" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Resource | Status | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|----------|--------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| â˜ï¸ CloudFront | âœ… Active | $DIST |" >> $GITHUB_STEP_SUMMARY
          echo "| ğŸ“¦ S3 Bucket | âœ… Active | $BUCKET |" >> $GITHUB_STEP_SUMMARY
          echo "| ğŸ›¡ï¸ WAF ACL | âœ… Active | $(echo $WAF | cut -d'/' -f2-) |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "### ğŸ§ª Health Checks" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Check | Result | Details |" >> $GITHUB_STEP_SUMMARY
          echo "|-------|--------|---------|" >> $GITHUB_STEP_SUMMARY
          
          if [ "$DOMAIN" != "N/A" ]; then
            URL="https://$DOMAIN"
            HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" --max-time 10 "$URL" 2>/dev/null || echo "000")
            
            if [ "$HTTP_CODE" = "200" ]; then
              echo "| ğŸŒ Website Access | âœ… Success | HTTP $HTTP_CODE |" >> $GITHUB_STEP_SUMMARY
            elif [ "$HTTP_CODE" = "000" ]; then
              echo "| ğŸŒ Website Access | â³ Propagating | Wait 5-10 minutes |" >> $GITHUB_STEP_SUMMARY
            else
              echo "| ğŸŒ Website Access | âš ï¸ HTTP $HTTP_CODE | May be propagating |" >> $GITHUB_STEP_SUMMARY
            fi
            
            HEADERS=$(curl -sI "$URL" --max-time 10 2>/dev/null || echo "")
            if echo "$HEADERS" | grep -qi "x-amz-cf-id"; then
              echo "| â˜ï¸ CloudFront | âœ… Serving | Requests via CDN |" >> $GITHUB_STEP_SUMMARY
            fi
            
            if echo "$HEADERS" | grep -qi "strict-transport-security"; then
              echo "| ğŸ”’ Security Headers | âœ… Configured | HSTS enabled |" >> $GITHUB_STEP_SUMMARY
            else
              echo "| ğŸ”’ Security Headers | â³ Propagating | Will appear shortly |" >> $GITHUB_STEP_SUMMARY
            fi
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### ğŸ“ˆ Monitoring & Management" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Service | Console Link |" >> $GITHUB_STEP_SUMMARY
          echo "|---------|--------------|" >> $GITHUB_STEP_SUMMARY
          echo "| â˜ï¸ CloudFront | [View Distribution](https://console.aws.amazon.com/cloudfront/v3/home#/distributions/$DIST) |" >> $GITHUB_STEP_SUMMARY
          echo "| ğŸ“¦ S3 Bucket | [View Bucket](https://s3.console.aws.amazon.com/s3/buckets/$BUCKET) |" >> $GITHUB_STEP_SUMMARY
          
          if [ "$DASHBOARD" != "N/A" ]; then
            echo "| ğŸ“Š CloudWatch | [View Dashboard]($DASHBOARD) |" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "| ğŸ›¡ï¸ WAF & Shield | [View Web ACLs](https://console.aws.amazon.com/wafv2/homev2/web-acls?region=global) |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### ğŸŒ Your Website" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**ğŸ‰ Access your website at:** https://$DOMAIN" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "ğŸ’¡ **Note:** CloudFront can take 10-15 minutes to fully propagate worldwide." >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### âœ… Next Steps" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… Infrastructure deployed/updated successfully" >> $GITHUB_STEP_SUMMARY
          echo "- ğŸŒ Test your website using the CloudFront URL above" >> $GITHUB_STEP_SUMMARY
          echo "- ğŸ“Š Monitor performance via CloudWatch dashboard" >> $GITHUB_STEP_SUMMARY
          echo "- ğŸ›¡ï¸ Review security settings in WAF console" >> $GITHUB_STEP_SUMMARY
          echo "- ğŸ“ Upload custom content to the S3 bucket" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### ğŸ”„ Resource Management" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- ğŸ§¹ Orphaned resources automatically cleaned" >> $GITHUB_STEP_SUMMARY
          echo "- ğŸ”„ Existing resources imported and reused" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… Same CloudFront URL on every deployment" >> $GITHUB_STEP_SUMMARY
          echo "- ğŸ’° No duplicate resources wasting costs" >> $GITHUB_STEP_SUMMARY